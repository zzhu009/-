{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beec8d7e-2c4b-4145-b538-ca68ddc2aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取第 1 页...\n",
      "正在爬取第 2 页...\n",
      "正在爬取第 3 页...\n",
      "爬取完成! 共获取 180 件商品数据\n",
      "数据已保存至: amazon_eco_products_20250530-202714.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 设置请求头模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "def scrape_amazon_eco_products(pages=3):\n",
    "    \"\"\"爬取多页亚马逊环保商品数据\"\"\"\n",
    "    base_url = \"https://www.amazon.com/s?k=eco+friendly&i=aps\"\n",
    "    products = []\n",
    "    \n",
    "    for page in range(1, pages+1):\n",
    "        print(f\"正在爬取第 {page} 页...\")\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            items = soup.select('div[data-component-type=\"s-search-result\"]')\n",
    "            \n",
    "            for item in items:\n",
    "                # 提取商品基本信息\n",
    "                title_elem = item.select_one('h2 a span')\n",
    "                title = title_elem.text.strip() if title_elem else None\n",
    "                \n",
    "                price_elem = item.select_one('.a-price .a-offscreen')\n",
    "                price = price_elem.text.strip() if price_elem else None\n",
    "                \n",
    "                rating_elem = item.select_one('i.a-icon-star-small .a-icon-alt')\n",
    "                rating = rating_elem.text.split()[0] if rating_elem else None\n",
    "                \n",
    "                review_elem = item.select_one('span.a-size-base')\n",
    "                reviews = review_elem.text.replace(',', '') if review_elem else \"0\"\n",
    "                \n",
    "                # 提取商品ASIN用于构建详情页链接\n",
    "                asin = item.get('data-asin')\n",
    "                detail_url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "                \n",
    "                # 获取商品详情页数据\n",
    "                product_detail = get_product_details(detail_url)\n",
    "                \n",
    "                products.append({\n",
    "                    'title': title,\n",
    "                    'price': price,\n",
    "                    'rating': rating,\n",
    "                    'reviews': reviews,\n",
    "                    'asin': asin,\n",
    "                    'url': detail_url,\n",
    "                    **product_detail\n",
    "                })\n",
    "                \n",
    "                # 随机延迟防止被封\n",
    "                time.sleep(random.uniform(1.0, 3.0))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"第 {page} 页爬取出错: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "def get_product_details(url):\n",
    "    \"\"\"获取商品详情页信息\"\"\"\n",
    "    details = {\n",
    "        'description': None,\n",
    "        'features': [],\n",
    "        'eco_certifications': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 提取商品描述\n",
    "        description_elem = soup.select_one('#productDescription')\n",
    "        if description_elem:\n",
    "            details['description'] = description_elem.text.strip()\n",
    "        \n",
    "        # 提取产品特性\n",
    "        feature_bullets = soup.select('#feature-bullets li')\n",
    "        details['features'] = [li.text.strip() for li in feature_bullets]\n",
    "        \n",
    "        # 识别环保认证关键词\n",
    "        eco_keywords = ['organic', 'eco-friendly', 'sustainable', 'recycled', \n",
    "                       'biodegradable', 'certified', 'green', 'energy star']\n",
    "        text_content = soup.get_text().lower()\n",
    "        \n",
    "        for keyword in eco_keywords:\n",
    "            if keyword in text_content:\n",
    "                details['eco_certifications'].append(keyword)\n",
    "        \n",
    "        # 提取技术规格\n",
    "        tech_specs = {}\n",
    "        for row in soup.select('#productDetails_techSpec_section_1 tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) == 2:\n",
    "                key = cols[0].text.strip().lower()\n",
    "                value = cols[1].text.strip()\n",
    "                tech_specs[key] = value\n",
    "        \n",
    "        details['technical_specs'] = tech_specs\n",
    "        \n",
    "        # 随机延迟\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"详情页爬取出错: {str(e)}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "# 执行爬虫\n",
    "if __name__ == \"__main__\":\n",
    "    # 爬取3页数据（约60件商品）\n",
    "    eco_df = scrape_amazon_eco_products(pages=3)\n",
    "    \n",
    "    # 保存数据到CSV\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f\"amazon_eco_products_{timestamp}.csv\"\n",
    "    eco_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"爬取完成! 共获取 {len(eco_df)} 件商品数据\")\n",
    "    print(f\"数据已保存至: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31ea07-0ba2-491e-92f5-a011678abea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
